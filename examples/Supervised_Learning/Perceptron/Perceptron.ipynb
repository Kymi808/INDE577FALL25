{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f333c62",
   "metadata": {},
   "source": [
    "## Perceptron — End-to-End Workflow (Binary, Linearly Separable Data)\n",
    "\n",
    "This notebook builds a full supervised binary classification pipeline using the classic Perceptron:\n",
    "\t•\tData Exploration: generate, inspect, visualize a (nearly) linearly separable dataset\n",
    "\t•\tPreprocessing: stratified split, (optional) scaling\n",
    "\t•\tModeling: from-scratch Perceptron + scikit-learn Perceptron, plus a Logistic Regression baseline\n",
    "\t•\tEvaluation: accuracy, precision/recall/F1, confusion matrix, decision regions\n",
    "\n",
    "Perceptron vs. MLP: Perceptron is a single linear layer with a step activation and converges only on linearly separable data. MLPs add hidden layers + nonlinearities (e.g., ReLU) and can model nonlinear boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b352514",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9783a3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & reproducibility\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Optional, Tuple, Union, Dict, List\n",
    "from IPython.display import display\n",
    "\n",
    "# sklearn utilities\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "# Optional viz\n",
    "try:\n",
    "    import seaborn as sns\n",
    "except Exception:\n",
    "    sns = None\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.rcParams[\"figure.figsize\"] = (6.5, 4.0)\n",
    "plt.rcParams[\"axes.grid\"] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e487a6",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "We’ll create a 2D, almost linearly separable dataset so we can visualize decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c199f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a (nearly) linearly separable 2D dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=800,\n",
    "    n_features=2,\n",
    "    n_informative=2,\n",
    "    n_redundant=0,\n",
    "    n_clusters_per_class=1,\n",
    "    class_sep=2.0,       # bigger = more separable\n",
    "    flip_y=0.02,         # small label noise\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(X, columns=[\"x1\", \"x2\"])\n",
    "df[\"target\"] = y\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "display(df.head())\n",
    "display(df.describe())\n",
    "\n",
    "# Quick scatter plot\n",
    "plt.scatter(df[\"x1\"], df[\"x2\"], c=df[\"target\"], cmap=\"bwr\", alpha=0.6, edgecolor=\"k\")\n",
    "plt.title(\"Synthetic data (2D)\")\n",
    "plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "print(\"\\nClass counts:\")\n",
    "display(df[\"target\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e72b5ca",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "We’ll do a stratified split. Perceptron doesn’t require scaling, but it often converges faster with standardized features, so we’ll include scaling in pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b997f424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test split (stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[[\"x1\", \"x2\"]], df[\"target\"], test_size=0.25, stratify=df[\"target\"], random_state=42\n",
    ")\n",
    "print(\"Train:\", X_train.shape, \" Test:\", X_test.shape)\n",
    "print(\"Train class balance:\\n\", y_train.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f9061a",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "We’ll train:\n",
    "\t1.\tFrom-scratch Perceptron (step activation, online updates on mistakes)\n",
    "\t2.\tsklearn Perceptron in a Pipeline(StandardScaler → Perceptron)\n",
    "\t3.\tLogistic Regression baseline (to compare a probabilistic linear classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08b6aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePerceptron:\n",
    "    \"\"\"\n",
    "    Classic binary perceptron (for teaching).\n",
    "    - Labels are internally mapped to {-1, +1}\n",
    "    - Online updates on misclassified points: w <- w + lr * y * x, b <- b + lr * y\n",
    "    - fit_intercept=True uses a separate bias term\n",
    "    \"\"\"\n",
    "    def __init__(self, lr: float = 1.0, max_iter: int = 1000, shuffle: bool = True, fit_intercept: bool = True, random_state: Optional[int] = 42):\n",
    "        self.lr = float(lr)\n",
    "        self.max_iter = int(max_iter)\n",
    "        self.shuffle = bool(shuffle)\n",
    "        self.fit_intercept = bool(fit_intercept)\n",
    "        self.random_state = random_state\n",
    "\n",
    "        self.coef_: Optional[np.ndarray] = None  # (d,)\n",
    "        self.intercept_: float = 0.0\n",
    "        self.classes_: Optional[np.ndarray] = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _sign(z: np.ndarray) -> np.ndarray:\n",
    "        # Perceptron convention: sign(0) -> +1\n",
    "        out = np.ones_like(z, dtype=int)\n",
    "        out[z < 0] = -1\n",
    "        return out\n",
    "\n",
    "    def fit(self, X: Union[pd.DataFrame, np.ndarray], y: Union[pd.Series, np.ndarray]):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        y = np.asarray(y)\n",
    "        n, d = X.shape\n",
    "\n",
    "        # Map labels to {-1, +1}\n",
    "        self.classes_ = np.unique(y)\n",
    "        assert self.classes_.size == 2, \"This simple implementation is binary only.\"\n",
    "        y_map = np.where(y == self.classes_[0], -1, +1)  # classes_[0] -> -1, classes_[1] -> +1\n",
    "\n",
    "        rng = np.random.RandomState(self.random_state)\n",
    "        self.coef_ = np.zeros(d, dtype=float)\n",
    "        self.intercept_ = 0.0\n",
    "\n",
    "        idx = np.arange(n)\n",
    "        for _ in range(self.max_iter):\n",
    "            if self.shuffle:\n",
    "                rng.shuffle(idx)\n",
    "            errors = 0\n",
    "            for i in idx:\n",
    "                xi = X[i]\n",
    "                yi = y_map[i]\n",
    "                z = np.dot(self.coef_, xi) + (self.intercept_ if self.fit_intercept else 0.0)\n",
    "                pred = +1 if z >= 0 else -1\n",
    "                if pred != yi:\n",
    "                    # update on mistakes\n",
    "                    self.coef_ += self.lr * yi * xi\n",
    "                    if self.fit_intercept:\n",
    "                        self.intercept_ += self.lr * yi\n",
    "                    errors += 1\n",
    "            if errors == 0:\n",
    "                break\n",
    "        return self\n",
    "\n",
    "    def decision_function(self, X: Union[pd.DataFrame, np.ndarray]) -> np.ndarray:\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        return X @ self.coef_ + (self.intercept_ if self.fit_intercept else 0.0)\n",
    "\n",
    "    def predict(self, X: Union[pd.DataFrame, np.ndarray]) -> np.ndarray:\n",
    "        z = self.decision_function(X)\n",
    "        # map back to original labels\n",
    "        signs = np.where(z >= 0, +1, -1)\n",
    "        return np.where(signs == -1, self.classes_[0], self.classes_[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373ab1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) From-scratch perceptron\n",
    "perc_scratch = SimplePerceptron(lr=1.0, max_iter=1000, shuffle=True, fit_intercept=True, random_state=42)\n",
    "perc_scratch.fit(X_train, y_train)\n",
    "\n",
    "# 2) sklearn perceptron with scaling\n",
    "perc_sklearn = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"perc\", Perceptron(\n",
    "        penalty=None,          # try None or 'l2'\n",
    "        alpha=0.0001,\n",
    "        max_iter=1000,\n",
    "        tol=1e-4,\n",
    "        eta0=1.0,\n",
    "        fit_intercept=True,\n",
    "        shuffle=True,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "perc_sklearn.fit(X_train, y_train)\n",
    "\n",
    "# 3) Logistic Regression baseline (scaled)\n",
    "logreg = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"lr\", LogisticRegression(\n",
    "        penalty=\"l2\", C=1.0, solver=\"lbfgs\", max_iter=2000, random_state=42\n",
    "    ))\n",
    "]).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912cb060",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning (sklearn perceptron)\n",
    "\n",
    "We’ll sweep a small grid for penalty strength and learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30299d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"perc\", Perceptron(max_iter=1000, tol=1e-4, random_state=42))\n",
    "    ]),\n",
    "    param_grid={\n",
    "        \"perc__penalty\": [None, \"l2\"],\n",
    "        \"perc__alpha\": [1e-4, 1e-3, 1e-2],\n",
    "        \"perc__eta0\": [0.1, 1.0, 2.0],\n",
    "        \"perc__fit_intercept\": [True, False],\n",
    "        # you can also test 'l1' or 'elasticnet' if desired\n",
    "    },\n",
    "    scoring=\"accuracy\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"Best CV accuracy:\", grid.best_score_)\n",
    "perc_best = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61ab00a",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We’ll compute accuracy, precision, recall, F1, and a confusion matrix for each model.\n",
    "We’ll also plot decision regions (2D) for visual intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb610c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_binary(name: str, model, X_te, y_te):\n",
    "    y_pred = model.predict(X_te)\n",
    "    acc = accuracy_score(y_te, y_pred)\n",
    "    prec = precision_score(y_te, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_te, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_te, y_pred, zero_division=0)\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {prec:.4f} | Recall: {rec:.4f} | F1: {f1:.4f}\")\n",
    "    print(\"Confusion matrix:\\n\", confusion_matrix(y_te, y_pred))\n",
    "    print(\"Classification report:\\n\", classification_report(y_te, y_pred))\n",
    "    return {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1}\n",
    "\n",
    "metrics = {}\n",
    "metrics[\"Perceptron (scratch)\"] = evaluate_binary(\"Perceptron (scratch)\", perc_scratch, X_test, y_test)\n",
    "metrics[\"Perceptron (sklearn)\"] = evaluate_binary(\"Perceptron (sklearn)\", perc_sklearn, X_test, y_test)\n",
    "metrics[\"Perceptron (tuned)\"] = evaluate_binary(\"Perceptron (tuned)\", perc_best, X_test, y_test)\n",
    "metrics[\"LogReg (baseline)\"] = evaluate_binary(\"LogReg (baseline)\", logreg, X_test, y_test)\n",
    "\n",
    "display(pd.DataFrame(metrics).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0d81bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D decision region plotting\n",
    "def plot_decision_regions_2d(model, X2: pd.DataFrame, y2: pd.Series, title: str):\n",
    "    X_np = X2.to_numpy()\n",
    "    y_np = y2.to_numpy()\n",
    "\n",
    "    # If model is a pipeline w/ scaler, we can fit a clone; here assume already fit.\n",
    "    x1_min, x1_max = X_np[:, 0].min() - 0.5, X_np[:, 0].max() + 0.5\n",
    "    x2_min, x2_max = X_np[:, 1].min() - 0.5, X_np[:, 1].max() + 0.5\n",
    "\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.linspace(x1_min, x1_max, 400),\n",
    "        np.linspace(x2_min, x2_max, 400),\n",
    "    )\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = model.predict(grid).reshape(xx.shape)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.contourf(xx, yy, Z, alpha=0.25, cmap=\"bwr\")\n",
    "    plt.scatter(X_np[:, 0], X_np[:, 1], c=y_np, cmap=\"bwr\", edgecolor=\"k\", alpha=0.7)\n",
    "    plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "plot_decision_regions_2d(perc_scratch, X_test, y_test, \"Decision regions — Perceptron (scratch)\")\n",
    "plot_decision_regions_2d(perc_sklearn, X_test, y_test, \"Decision regions — Perceptron (sklearn)\")\n",
    "plot_decision_regions_2d(perc_best, X_test, y_test, \"Decision regions — Perceptron (tuned)\")\n",
    "plot_decision_regions_2d(logreg, X_test, y_test, \"Decision regions — Logistic Regression (baseline)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17644c9f",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\t•\tThe Perceptron learns a linear decision boundary with a step activation. It converges only if the training data are linearly separable (otherwise it may cycle).\n",
    "\t•\tScaling can speed convergence; shuffling and learning rate (eta0 / lr) affect stability.\n",
    "\t•\tIt does not produce probabilities; for calibrated probabilities use Logistic Regression or SVM (Platt scaling).\n",
    "\t•\tUseful as a baseline or online learner; for nonlinear boundaries consider MLP (with hidden layers) or kernel methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
