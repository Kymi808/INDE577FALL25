{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "921347e0",
   "metadata": {},
   "source": [
    "## Ensemble Methods — Bagging, Random Forests, and Boosting\n",
    "\n",
    "This notebook walks through an end-to-end supervised learning pipeline with Ensemble Methods:\n",
    "\t•\tData Exploration: load, inspect, visualize\n",
    "\t•\tPreprocessing: split, (optional) imputation\n",
    "\t•\tModeling: from-scratch Bagging (+ sklearn RandomForest, AdaBoost, GradientBoosting)\n",
    "\t•\tEvaluation: accuracy, confusion matrix, classification report, ROC-AUC, ROC curves, feature importances\n",
    "\t•\tTuning: small GridSearchCV for RandomForest/GradientBoosting\n",
    "\n",
    "Concepts:\n",
    "\t•\tBagging averages/votes across many high-variance learners trained on bootstrap samples.\n",
    "\t•\tRandom Forest = Bagging with decision trees + random feature subspacing per split.\n",
    "\t•\tBoosting fits learners sequentially, focusing on previously misclassified points (AdaBoost) or optimizing a differentiable loss (Gradient Boosting)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3616804f",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We’ll use the Breast Cancer Wisconsin dataset (binary classification). Ensembles don’t require feature scaling; we include imputation for robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04caaac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & reproducibility\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Optional, Tuple, Union, List, Dict\n",
    "from IPython.display import display\n",
    "\n",
    "# sklearn utilities\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "\n",
    "# Optional viz\n",
    "try:\n",
    "    import seaborn as sns\n",
    "except Exception:\n",
    "    sns = None\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.rcParams[\"figure.figsize\"] = (6.5, 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cb4837",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "We’ll load the dataset, examine shapes, summary statistics, and class balance. Optional: quick correlations heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8f8cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Breast Cancer dataset\n",
    "cancer = load_breast_cancer(as_frame=True)\n",
    "X: pd.DataFrame = cancer.data.copy()\n",
    "y: pd.Series = pd.Series(cancer.target, name=\"target\")  # 0=malignant, 1=benign\n",
    "\n",
    "df = X.copy()\n",
    "df[\"target\"] = y\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "display(df.head())\n",
    "display(df.describe())\n",
    "\n",
    "print(\"\\nClass counts (0=malignant, 1=benign):\")\n",
    "display(y.value_counts())\n",
    "\n",
    "# Optional correlations heatmap (may be dense)\n",
    "if sns is not None:\n",
    "    try:\n",
    "        corr = df.corr(numeric_only=True)\n",
    "        sns.heatmap(corr, cmap=\"coolwarm\", center=0)\n",
    "        plt.title(\"Feature Correlations (numeric)\")\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(\"Skipping heatmap:\", e)\n",
    "else:\n",
    "    print(\"Seaborn not installed; skipping heatmap.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4758ad72",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\t•\tStratified train/test split to preserve class proportions\n",
    "\t•\tTrees/ensembles don’t need scaling; we’ll keep a median imputer in each pipeline for generality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cb1121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test split (stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, stratify=y, random_state=42\n",
    ")\n",
    "print(\"Train shape:\", X_train.shape, \" Test shape:\", X_test.shape)\n",
    "print(\"Train class balance:\\n\", y_train.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db7d79c",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "We’ll train four models:\n",
    "\t1.\tFrom-scratch Bagging classifier (simple, pedagogical) using decision trees as base learners\n",
    "\t2.\tRandomForestClassifier (bagging + random subspace)\n",
    "\t3.\tAdaBoostClassifier (boosting with exponential loss)\n",
    "\t4.\tGradientBoostingClassifier (boosting with differentiable loss)\n",
    "\n",
    "We’ll also do a small GridSearchCV for RandomForest and GradientBoosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee4961f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBaggingClassifier:\n",
    "    \"\"\"\n",
    "    A minimal Bagging classifier using a base estimator (default: DecisionTreeClassifier).\n",
    "    - Fits `n_estimators` learners on bootstrap samples (rows) and optional feature subsamples (columns).\n",
    "    - Aggregates by probability averaging (then argmax).\n",
    "    - Computes an OOB accuracy if bootstrap=True.\n",
    "    NOTE: This is for teaching; for production, use sklearn.ensemble.BaggingClassifier.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_estimator: Optional[DecisionTreeClassifier] = None,\n",
    "        n_estimators: int = 50,\n",
    "        max_samples: float = 0.8,     # fraction of rows per bootstrap sample\n",
    "        max_features: float = 1.0,    # fraction of columns per estimator\n",
    "        bootstrap: bool = True,\n",
    "        random_state: Optional[int] = 42,\n",
    "    ):\n",
    "        self.base_estimator = base_estimator or DecisionTreeClassifier(\n",
    "            max_depth=3, random_state=0\n",
    "        )\n",
    "        self.n_estimators = int(n_estimators)\n",
    "        self.max_samples = float(max_samples)\n",
    "        self.max_features = float(max_features)\n",
    "        self.bootstrap = bool(bootstrap)\n",
    "        self.random_state = random_state\n",
    "\n",
    "        self.estimators_: List[DecisionTreeClassifier] = []\n",
    "        self.feature_indices_: List[np.ndarray] = []\n",
    "        self.classes_: Optional[np.ndarray] = None\n",
    "        self.oob_score_: Optional[float] = None\n",
    "\n",
    "    def fit(self, X: Union[pd.DataFrame, np.ndarray], y: Union[pd.Series, np.ndarray]):\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        n, d = X.shape\n",
    "        rng = np.random.RandomState(self.random_state)\n",
    "\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.estimators_.clear()\n",
    "        self.feature_indices_.clear()\n",
    "\n",
    "        # Track in-bag indices for OOB score\n",
    "        inbag_masks = []\n",
    "\n",
    "        n_rows = max(1, int(round(self.max_samples * n)))\n",
    "        n_cols = max(1, int(round(self.max_features * d)))\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            # sample rows\n",
    "            if self.bootstrap:\n",
    "                row_idx = rng.randint(0, n, size=n_rows)  # with replacement\n",
    "            else:\n",
    "                row_idx = rng.choice(np.arange(n), size=n_rows, replace=False)\n",
    "\n",
    "            # sample columns\n",
    "            feat_idx = rng.choice(np.arange(d), size=n_cols, replace=False)\n",
    "\n",
    "            est = self._fresh_estimator(i)\n",
    "            est.fit(X[row_idx][:, feat_idx], y[row_idx])\n",
    "\n",
    "            self.estimators_.append(est)\n",
    "            self.feature_indices_.append(feat_idx)\n",
    "\n",
    "            mask = np.zeros(n, dtype=bool)\n",
    "            mask[row_idx] = True\n",
    "            inbag_masks.append(mask)\n",
    "\n",
    "        # Compute OOB score (if any OOB per sample exists)\n",
    "        if self.bootstrap:\n",
    "            votes_sum = np.zeros((n, len(self.classes_)), dtype=float)\n",
    "            votes_cnt = np.zeros(n, dtype=int)\n",
    "\n",
    "            class_to_pos = {c: j for j, c in enumerate(self.classes_)}\n",
    "            for est, feat_idx, inbag in zip(self.estimators_, self.feature_indices_, inbag_masks):\n",
    "                oob_idx = np.where(~inbag)[0]\n",
    "                if oob_idx.size == 0:\n",
    "                    continue\n",
    "                proba = self._predict_proba_with_est(est, X[oob_idx][:, feat_idx], class_to_pos)\n",
    "                votes_sum[oob_idx] += proba\n",
    "                votes_cnt[oob_idx] += 1\n",
    "\n",
    "            usable = votes_cnt > 0\n",
    "            if usable.any():\n",
    "                y_oob_pred = self.classes_[np.argmax(votes_sum[usable] / votes_cnt[usable, None], axis=1)]\n",
    "                self.oob_score_ = accuracy_score(y[usable], y_oob_pred)\n",
    "            else:\n",
    "                self.oob_score_ = None\n",
    "        else:\n",
    "            self.oob_score_ = None\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: Union[pd.DataFrame, np.ndarray]) -> np.ndarray:\n",
    "        proba = self.predict_proba(X)\n",
    "        return self.classes_[np.argmax(proba, axis=1)]\n",
    "\n",
    "    def predict_proba(self, X: Union[pd.DataFrame, np.ndarray]) -> np.ndarray:\n",
    "        X = np.asarray(X)\n",
    "        n = X.shape[0]\n",
    "        proba_sum = np.zeros((n, len(self.classes_)), dtype=float)\n",
    "        class_to_pos = {c: j for j, c in enumerate(self.classes_)}\n",
    "\n",
    "        for est, feat_idx in zip(self.estimators_, self.feature_indices_):\n",
    "            proba_sum += self._predict_proba_with_est(est, X[:, feat_idx], class_to_pos)\n",
    "\n",
    "        return proba_sum / float(self.n_estimators)\n",
    "\n",
    "    # ---- helpers ----\n",
    "    def _fresh_estimator(self, i: int) -> DecisionTreeClassifier:\n",
    "        # Create a new base estimator per iteration with a different random_state for diversity\n",
    "        rs = None if self.base_estimator.random_state is None else (self.base_estimator.random_state + i + 1)\n",
    "        est = DecisionTreeClassifier(\n",
    "            criterion=getattr(self.base_estimator, \"criterion\", \"gini\"),\n",
    "            max_depth=self.base_estimator.max_depth,\n",
    "            min_samples_leaf=self.base_estimator.min_samples_leaf if hasattr(self.base_estimator, \"min_samples_leaf\") else 1,\n",
    "            random_state=rs,\n",
    "        )\n",
    "        return est\n",
    "\n",
    "    def _predict_proba_with_est(\n",
    "        self,\n",
    "        est: DecisionTreeClassifier,\n",
    "        X_sub: np.ndarray,\n",
    "        class_to_pos: Dict[Union[int, float], int],\n",
    "    ) -> np.ndarray:\n",
    "        # Align estimator's class order to global classes_\n",
    "        est_proba = est.predict_proba(X_sub)\n",
    "        aligned = np.zeros((X_sub.shape[0], len(self.classes_)), dtype=float)\n",
    "        for j, c in enumerate(est.classes_):\n",
    "            aligned[:, class_to_pos[c]] = est_proba[:, j]\n",
    "        return aligned"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
