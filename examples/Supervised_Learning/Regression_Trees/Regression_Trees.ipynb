{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bfc4022",
   "metadata": {},
   "source": [
    "## Regression Trees — End-to-End Workflow\n",
    "\n",
    "We’ll build a supervised regression pipeline with Regression Trees (CART):\n",
    "\t•\tData Exploration: load, inspect, visualize\n",
    "\t•\tPreprocessing: train/test split, (optional) imputation\n",
    "\t•\tModeling: from-scratch SimpleDecisionTreeRegressor and scikit-learn DecisionTreeRegressor, plus tuning\n",
    "\t•\tEvaluation: MAE, MSE, RMSE, R^2, residual plots, parity plots, feature importances, and tree text rules\n",
    "\n",
    "Concept: A regression tree recursively splits the feature space to minimize within-node squared error (variance), predicting the mean target in each leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a61031e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c82b4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & reproducibility\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Optional, Tuple, Union, List, Dict\n",
    "from IPython.display import display\n",
    "\n",
    "# sklearn utilities\n",
    "from sklearn.datasets import fetch_california_housing, load_diabetes\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import DecisionTreeRegressor, export_text\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Optional viz\n",
    "try:\n",
    "    import seaborn as sns\n",
    "except Exception:\n",
    "    sns = None\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.rcParams[\"figure.figsize\"] = (6.8, 4.2)\n",
    "plt.rcParams[\"axes.grid\"] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dacea3",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "We’ll try California Housing (bigger, nicer), and fallback to Diabetes if fetching isn’t available. We’ll inspect shapes, summaries, target distribution, and correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c3a9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (California Housing -> fallback to Diabetes)\n",
    "dataset_name = \"California Housing\"\n",
    "try:\n",
    "    cali = fetch_california_housing(as_frame=True)\n",
    "    X: pd.DataFrame = cali.data.copy()\n",
    "    y: pd.Series = pd.Series(cali.target, name=\"target\")  # median house value (in 100k$)\n",
    "except Exception as e:\n",
    "    print(\"Could not fetch California Housing; falling back to Diabetes. Reason:\", e)\n",
    "    dataset_name = \"Diabetes\"\n",
    "    diab = load_diabetes(as_frame=True)\n",
    "    X = diab.data.copy()\n",
    "    y = pd.Series(diab.target, name=\"target\")\n",
    "\n",
    "df = X.copy()\n",
    "df[\"target\"] = y\n",
    "\n",
    "print(f\"Dataset: {dataset_name}\")\n",
    "print(\"Shape:\", df.shape)\n",
    "display(df.head())\n",
    "display(df.describe())\n",
    "\n",
    "# Target distribution\n",
    "plt.hist(y, bins=30, edgecolor=\"k\", alpha=0.8)\n",
    "plt.title(\"Target distribution\")\n",
    "plt.xlabel(\"target\"); plt.ylabel(\"count\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Correlations with target\n",
    "corr_t = df.corr(numeric_only=True)[\"target\"].sort_values(ascending=False)\n",
    "display(corr_t.to_frame(\"corr_with_target\"))\n",
    "\n",
    "# Optional full heatmap\n",
    "if sns is not None:\n",
    "    try:\n",
    "        sns.heatmap(df.corr(numeric_only=True), cmap=\"coolwarm\", center=0)\n",
    "        plt.title(\"Correlation heatmap\")\n",
    "        plt.tight_layout(); plt.show()\n",
    "    except Exception as e:\n",
    "        print(\"Skipping heatmap:\", e)\n",
    "else:\n",
    "    print(\"Seaborn not installed; skipping heatmap.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73731888",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\t•\tTrain/Test split (regression → no stratify)\n",
    "\t•\tTrees don’t need scaling; we’ll keep a median imputer for robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4b5b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42\n",
    ")\n",
    "print(\"Train:\", X_train.shape, \" Test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6a6379",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "We’ll train:\n",
    "\t1.\tFrom-scratch Regression Tree (variance reduction / MSE)\n",
    "\t2.\tscikit-learn DecisionTreeRegressor in a Pipeline\n",
    "We’ll also run a small GridSearchCV to tune depth/leaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705749cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class _RNode:\n",
    "    mse: float\n",
    "    num_samples: int\n",
    "    value: float                    # mean target in the node\n",
    "    feature_index: Optional[int] = None\n",
    "    threshold: Optional[float] = None\n",
    "    left: Optional[\"_RNode\"] = None\n",
    "    right: Optional[\"_RNode\"] = None\n",
    "\n",
    "class SimpleDecisionTreeRegressor:\n",
    "    \"\"\"\n",
    "    Minimal CART-style Regression Tree using MSE (variance) reduction.\n",
    "    - Splits choose feature/threshold that minimize weighted MSE of children.\n",
    "    - Leaf prediction = mean(y) in leaf.\n",
    "    NOTE: Educational; for production, prefer sklearn's DecisionTreeRegressor.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_depth: Optional[int] = None,\n",
    "        min_samples_split: int = 2,\n",
    "         min_samples_leaf: int = 1,\n",
    "    ):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = int(min_samples_split)\n",
    "        self.min_samples_leaf = int(min_samples_leaf)\n",
    "        self.n_features_: Optional[int] = None\n",
    "        self.tree_: Optional[_RNode] = None\n",
    "\n",
    "    def fit(self, X: Union[pd.DataFrame, np.ndarray], y: Union[pd.Series, np.ndarray]):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        y = np.asarray(y, dtype=float)\n",
    "        self.n_features_ = X.shape[1]\n",
    "        self.tree_ = self._grow(X, y, depth=0)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: Union[pd.DataFrame, np.ndarray]) -> np.ndarray:\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        return np.array([self._predict_row(row) for row in X], dtype=float)\n",
    "\n",
    "    # ---- internals ----\n",
    "    def _node_mse(self, y: np.ndarray) -> float:\n",
    "        # mean squared error relative to the node's mean\n",
    "        if y.size == 0:\n",
    "            return 0.0\n",
    "        mu = y.mean()\n",
    "        return float(((y - mu) ** 2).mean())\n",
    "\n",
    "    def _best_split(self, X: np.ndarray, y: np.ndarray) -> Tuple[Optional[int], Optional[float]]:\n",
    "        m, n = X.shape\n",
    "        if m < self.min_samples_split:\n",
    "            return None, None\n",
    "\n",
    "        # Precompute totals for variance math\n",
    "        total_sum = y.sum()\n",
    "        total_sq_sum = (y ** 2).sum()\n",
    "\n",
    "        best_score = np.inf\n",
    "        best_idx, best_thr = None, None\n",
    "\n",
    "        for j in range(n):\n",
    "            order = np.argsort(X[:, j])\n",
    "            xj = X[order, j]\n",
    "            yj = y[order]\n",
    "\n",
    "            # cumulative sums for fast left/right stats\n",
    "            csum = 0.0\n",
    "            csum_sq = 0.0\n",
    "\n",
    "            for i in range(1, m):\n",
    "                yi_prev = yj[i - 1]\n",
    "                csum += yi_prev\n",
    "                csum_sq += yi_prev * yi_prev\n",
    "\n",
    "                # skip identical thresholds\n",
    "                if xj[i] == xj[i - 1]:\n",
    "                    continue\n",
    "\n",
    "                n_left = i\n",
    "                n_right = m - i\n",
    "                if n_left < self.min_samples_leaf or n_right < self.min_samples_leaf:\n",
    "                    continue\n",
    "\n",
    "                sum_left = csum\n",
    "                sum_right = total_sum - csum\n",
    "\n",
    "                sq_left = csum_sq\n",
    "                sq_right = total_sq_sum - csum_sq\n",
    "\n",
    "                # MSE = E[y^2] - (E[y])^2\n",
    "                mean_left = sum_left / n_left\n",
    "                mean_right = sum_right / n_right\n",
    "                mse_left = (sq_left / n_left) - (mean_left ** 2)\n",
    "                mse_right = (sq_right / n_right) - (mean_right ** 2)\n",
    "                weighted_mse = (n_left * mse_left + n_right * mse_right) / m\n",
    "\n",
    "                if weighted_mse < best_score:\n",
    "                    best_score = weighted_mse\n",
    "                    best_idx = j\n",
    "                    best_thr = 0.5 * (xj[i] + xj[i - 1])\n",
    "\n",
    "        return best_idx, best_thr\n",
    "\n",
    "    def _grow(self, X: np.ndarray, y: np.ndarray, depth: int) -> _RNode:\n",
    "        node = _RNode(\n",
    "            mse=self._node_mse(y),\n",
    "            num_samples=y.size,\n",
    "            value=float(y.mean()) if y.size else 0.0,\n",
    "        )\n",
    "        depth_ok = True if self.max_depth is None else (depth < self.max_depth)\n",
    "        if depth_ok:\n",
    "            idx, thr = self._best_split(X, y)\n",
    "            if idx is not None:\n",
    "                left_mask = X[:, idx] < float(thr)\n",
    "                X_l, y_l = X[left_mask], y[left_mask]\n",
    "                X_r, y_r = X[~left_mask], y[~left_mask]\n",
    "                if y_l.size and y_r.size:\n",
    "                    node.feature_index = idx\n",
    "                    node.threshold = float(thr)\n",
    "                    node.left = self._grow(X_l, y_l, depth + 1)\n",
    "                    node.right = self._grow(X_r, y_r, depth + 1)\n",
    "        return node\n",
    "\n",
    "    def _traverse(self, row: np.ndarray) -> _RNode:\n",
    "        node = self.tree_\n",
    "        while node and node.left is not None:\n",
    "            if row[node.feature_index] < node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node\n",
    "\n",
    "    def _predict_row(self, row: np.ndarray) -> float:\n",
    "        leaf = self._traverse(row)\n",
    "        return leaf.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe5e733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build models\n",
    "tree_scratch = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"tree\", SimpleDecisionTreeRegressor(\n",
    "        max_depth=6, min_samples_split=4, min_samples_leaf=2\n",
    "    ))\n",
    "])\n",
    "\n",
    "tree_sklearn = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"dtr\", DecisionTreeRegressor(\n",
    "        max_depth=None, random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "tree_scratch.fit(X_train, y_train)\n",
    "tree_sklearn.fit(X_train, y_train)\n",
    "print(\"Fitted: Simple (scratch) & sklearn trees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2c7029",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning (sklearn)\n",
    "\n",
    "We’ll run a compact grid over max_depth, min_samples_leaf, and min_samples_split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc011c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"dtr\", DecisionTreeRegressor(random_state=42))\n",
    "    ]),\n",
    "    param_grid={\n",
    "        \"dtr__max_depth\": [3, 5, 7, None],\n",
    "        \"dtr__min_samples_leaf\": [1, 3, 5, 10],\n",
    "        \"dtr__min_samples_split\": [2, 4, 8]\n",
    "    },\n",
    "    scoring=\"r2\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"Best CV R^2:\", grid.best_score_)\n",
    "tree_best = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd49f7b",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We’ll report MAE, MSE, RMSE, and R^2; show residual vs fitted and parity plots.\n",
    "We’ll also print the tree rules (sklearn) and feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c24406d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_report(name: str, y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"MAE:  {mae: .4f}\")\n",
    "    print(f\"MSE:  {mse: .4f}\")\n",
    "    print(f\"RMSE: {rmse: .4f}\")\n",
    "    print(f\"R^2:  {r2: .4f}\")\n",
    "    return {\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2\": r2}\n",
    "\n",
    "def plot_residuals(y_true: np.ndarray, y_pred: np.ndarray, title: str):\n",
    "    resid = y_true - y_pred\n",
    "    plt.scatter(y_pred, resid, edgecolor=\"k\", alpha=0.7)\n",
    "    plt.axhline(0, color=\"k\", lw=1)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Residual (y - ŷ)\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "def plot_parity(y_true: np.ndarray, y_pred: np.ndarray, title: str):\n",
    "    lims = [min(y_true.min(), y_pred.min()), max(y_true.max(), y_pred.max())]\n",
    "    plt.scatter(y_true, y_pred, edgecolor=\"k\", alpha=0.7)\n",
    "    plt.plot(lims, lims, \"k--\", lw=1)\n",
    "    plt.xlabel(\"True y\")\n",
    "    plt.ylabel(\"Predicted ŷ\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4ac77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred_scratch = tree_scratch.predict(X_test)\n",
    "y_pred_sklearn = tree_sklearn.predict(X_test)\n",
    "y_pred_best = tree_best.predict(X_test)\n",
    "\n",
    "# Reports\n",
    "metrics = {}\n",
    "metrics[\"Tree (scratch)\"] = regression_report(\"Tree (scratch)\", y_test.to_numpy(), y_pred_scratch)\n",
    "metrics[\"Tree (sklearn)\"] = regression_report(\"Tree (sklearn)\", y_test.to_numpy(), y_pred_sklearn)\n",
    "metrics[\"Tree (tuned)\"] = regression_report(\"Tree (tuned)\", y_test.to_numpy(), y_pred_best)\n",
    "\n",
    "display(pd.DataFrame(metrics).T)\n",
    "\n",
    "# Plots\n",
    "plot_residuals(y_test.to_numpy(), y_pred_sklearn, \"Residuals vs Fitted — sklearn tree\")\n",
    "plot_parity(y_test.to_numpy(), y_pred_sklearn, \"Parity plot — sklearn tree\")\n",
    "\n",
    "plot_residuals(y_test.to_numpy(), y_pred_best, \"Residuals vs Fitted — tuned tree\")\n",
    "plot_parity(y_test.to_numpy(), y_pred_best, \"Parity plot — tuned tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b95505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importances (only for sklearn trees)\n",
    "def plot_importances(model, step_name: str, feature_names: List[str], title: str):\n",
    "    try:\n",
    "        inner = model.named_steps[step_name]\n",
    "        importances = getattr(inner, \"feature_importances_\", None)\n",
    "        if importances is None:\n",
    "            print(f\"No feature_importances_ for {title}\")\n",
    "            return\n",
    "        imp = pd.Series(importances, index=feature_names).sort_values(ascending=False)\n",
    "        display(imp.to_frame(\"importance\").head(15))\n",
    "        imp.head(25).plot(kind=\"bar\")\n",
    "        plt.title(title)\n",
    "        plt.ylabel(\"Importance\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not plot importances for {title}:\", e)\n",
    "\n",
    "plot_importances(tree_sklearn, \"dtr\", list(X.columns), \"DecisionTreeRegressor Feature Importances\")\n",
    "plot_importances(tree_best, \"dtr\", list(X.columns), \"DecisionTreeRegressor (tuned) Feature Importances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82740be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Textual tree rules (limit depth for readability)\n",
    "try:\n",
    "    inner = tree_best.named_steps[\"dtr\"]\n",
    "    txt = export_text(inner, feature_names=list(X.columns), max_depth=3)\n",
    "    print(\"Tree (tuned) — first levels:\\n\")\n",
    "    print(txt)\n",
    "except Exception as e:\n",
    "    print(\"Could not export text tree:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054a5a75",
   "metadata": {},
   "source": [
    "## Depth vs Validation Curve\n",
    "\n",
    "Deeper trees can overfit. Let’s see cross-validated R^2 vs max_depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99978b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "depths = [2, 3, 4, 5, 6, 8, 10, None]\n",
    "scores = []\n",
    "for d in depths:\n",
    "    pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"dtr\", DecisionTreeRegressor(max_depth=d, random_state=42))\n",
    "    ])\n",
    "    cv_scores = cross_val_score(pipe, X_train, y_train, cv=KFold(5, shuffle=True, random_state=42), scoring=\"r2\", n_jobs=-1)\n",
    "    scores.append(cv_scores.mean())\n",
    "\n",
    "plt.plot([str(d) for d in depths], scores, marker=\"o\")\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel(\"CV R^2 (mean)\")\n",
    "plt.title(\"Depth vs Validation Performance\")\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a92905",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\t•\tRegression Trees split features to minimize within-node squared error; leaves predict the mean.\n",
    "\t•\tThey capture nonlinearities & interactions without feature scaling.\n",
    "\t•\tControl complexity with max_depth, min_samples_leaf, and min_samples_split; tune them with CV to avoid over/underfitting.\n",
    "\t•\tInspect residuals and parity plots; use feature importances and tree rules for interpretability.\n",
    "\t•\tFor better accuracy/robustness, try tree ensembles (Random Forests, Gradient Boosting)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
