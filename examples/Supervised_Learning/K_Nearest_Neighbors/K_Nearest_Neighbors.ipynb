{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39391f1e",
   "metadata": {},
   "source": [
    "## K Nearest Neighbors (KNN) — End-to-End Workflow\n",
    "\n",
    "This notebook walks through a full supervised learning pipeline for KNN:\n",
    "\t•\tData Exploration: load, inspect, visualize\n",
    "\t•\tPreprocessing: train/test split, feature scaling (critical for distance-based models)\n",
    "\t•\tModeling: from-scratch SimpleKNNClassifier and scikit-learn KNeighborsClassifier, plus hyperparameter tuning\n",
    "\t•\tEvaluation: accuracy, confusion matrix, classification report, ROC-AUC (OvR), k-selection curve\n",
    "\n",
    "Why KNN? A simple, non-parametric, instance-based learner that classifies a sample by the majority (or distance-weighted) vote of its k nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2899d186",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We’ll use the Wine dataset (multiclass classification, numeric features). KNN depends on distances → always scale features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592452f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & reproducibility\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Optional, Tuple, Union, List, Dict\n",
    "from IPython.display import display\n",
    "\n",
    "# sklearn utilities\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Optional viz\n",
    "try:\n",
    "    import seaborn as sns\n",
    "except Exception:\n",
    "    sns = None\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.rcParams[\"figure.figsize\"] = (6.5, 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef9be9d",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "We’ll load Wine, inspect shape and stats, check class balance, and (optionally) view quick pairwise structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ce4e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Wine dataset\n",
    "wine = load_wine(as_frame=True)\n",
    "X: pd.DataFrame = wine.data.copy()\n",
    "y: pd.Series = pd.Series(wine.target, name=\"target\")  # classes: 0,1,2\n",
    "\n",
    "df = X.copy()\n",
    "df[\"target\"] = y\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "display(df.head())\n",
    "display(df.describe())\n",
    "\n",
    "print(\"\\nClass counts (0,1,2):\")\n",
    "display(y.value_counts())\n",
    "\n",
    "# Optional overview via PCA (2D projection)\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_p2 = pca.fit_transform(X)\n",
    "plt.scatter(X_p2[:,0], X_p2[:,1], c=y, edgecolor=\"k\", alpha=0.8)\n",
    "plt.title(\"PCA projection (2D) of Wine features\")\n",
    "plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355a9b40",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\t•\tStratified train/test split\n",
    "\t•\tStandardize features (StandardScaler) — KNN is distance-based, so scaling matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbb731c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test split (stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, stratify=y, random_state=42\n",
    ")\n",
    "print(\"Train shape:\", X_train.shape, \" Test shape:\", X_test.shape)\n",
    "print(\"Train class proportions:\\n\", y_train.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355108d3",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "We’ll train:\n",
    "\t1.\tFrom-scratch KNN (Euclidean distance; uniform or distance weighting)\n",
    "\t2.\tscikit-learn KNN in a pipeline with scaling\n",
    "We’ll also tune k, weights, and metric with GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d634252",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleKNNClassifier:\n",
    "    \"\"\"\n",
    "    Minimal KNN classifier\n",
    "    - Euclidean distance\n",
    "    - weights: 'uniform' (majority vote) or 'distance' (1 / (d + eps))\n",
    "    - Stores training data (lazy learner)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_neighbors: int = 5, weights: str = \"uniform\", eps: float = 1e-12):\n",
    "        assert n_neighbors >= 1\n",
    "        assert weights in (\"uniform\", \"distance\")\n",
    "        self.n_neighbors = int(n_neighbors)\n",
    "        self.weights = weights\n",
    "        self.eps = float(eps)\n",
    "        self.X_: Optional[np.ndarray] = None\n",
    "        self.y_: Optional[np.ndarray] = None\n",
    "        self.classes_: Optional[np.ndarray] = None\n",
    "\n",
    "    def fit(self, X: Union[pd.DataFrame, np.ndarray], y: Union[pd.Series, np.ndarray]):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        y = np.asarray(y)\n",
    "        self.X_ = X\n",
    "        self.y_ = y\n",
    "        self.classes_ = np.unique(y)\n",
    "        return self\n",
    "\n",
    "    def _distances(self, x: np.ndarray) -> np.ndarray:\n",
    "        # Euclidean distances to all training points (vectorized)\n",
    "        # ||X - x|| across rows\n",
    "        diff = self.X_ - x\n",
    "        return np.sqrt(np.sum(diff * diff, axis=1))\n",
    "\n",
    "    def _vote(self, idx: np.ndarray, dists: np.ndarray) -> int:\n",
    "        # Uniform: count labels; Distance: weight by 1/(d+eps)\n",
    "        labels = self.y_[idx]\n",
    "        if self.weights == \"uniform\":\n",
    "            # majority vote; tie-break by smallest class id\n",
    "            counts = {c: 0.0 for c in self.classes_}\n",
    "            for lab in labels:\n",
    "                counts[lab] += 1.0\n",
    "        else:\n",
    "            # distance-weighted vote\n",
    "            counts = {c: 0.0 for c in self.classes_}\n",
    "            nn_d = dists[idx]\n",
    "            w = 1.0 / (nn_d + self.eps)\n",
    "            for lab, wv in zip(labels, w):\n",
    "                counts[lab] += wv\n",
    "\n",
    "        # return argmax with deterministic tie-break\n",
    "        best_lab = None\n",
    "        best_val = -np.inf\n",
    "        for c in sorted(self.classes_):  # sort ensures deterministic tie-break to smaller class id\n",
    "            if counts[c] > best_val:\n",
    "                best_val = counts[c]\n",
    "                best_lab = c\n",
    "        return int(best_lab)\n",
    "\n",
    "    def _proba(self, idx: np.ndarray, dists: np.ndarray) -> np.ndarray:\n",
    "        # Probability estimate by normalized (uniform or distance) weights\n",
    "        labels = self.y_[idx]\n",
    "        if self.weights == \"uniform\":\n",
    "            w = np.ones_like(idx, dtype=float)\n",
    "        else:\n",
    "            nn_d = dists[idx]\n",
    "            w = 1.0 / (nn_d + self.eps)\n",
    "\n",
    "        # sum weights per class\n",
    "        sums = np.zeros(len(self.classes_), dtype=float)\n",
    "        class_to_pos = {c: j for j, c in enumerate(self.classes_)}\n",
    "        for lab, wv in zip(labels, w):\n",
    "            sums[class_to_pos[lab]] += wv\n",
    "\n",
    "        if sums.sum() == 0:\n",
    "            # fallback uniform if all zero (should not really happen)\n",
    "            return np.ones_like(sums) / len(sums)\n",
    "        return sums / sums.sum()\n",
    "\n",
    "    def predict(self, X: Union[pd.DataFrame, np.ndarray]) -> np.ndarray:\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        preds = []\n",
    "        for i in range(X.shape[0]):\n",
    "            d = self._distances(X[i])\n",
    "            nn_idx = np.argsort(d)[: self.n_neighbors]\n",
    "            preds.append(self._vote(nn_idx, d))\n",
    "        return np.array(preds, dtype=int)\n",
    "\n",
    "    def predict_proba(self, X: Union[pd.DataFrame, np.ndarray]) -> np.ndarray:\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        probs = []\n",
    "        for i in range(X.shape[0]):\n",
    "            d = self._distances(X[i])\n",
    "            nn_idx = np.argsort(d)[: self.n_neighbors]\n",
    "            probs.append(self._proba(nn_idx, d))\n",
    "        return np.vstack(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c60eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build BOTH models, with proper scaling\n",
    "# 1) From-scratch path: fit scaler, transform arrays, then SimpleKNN\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "knn_simple = SimpleKNNClassifier(n_neighbors=5, weights=\"uniform\")\n",
    "knn_simple.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 2) sklearn KNN in a Pipeline (recommended for production)\n",
    "knn_sklearn = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"knn\", KNeighborsClassifier(n_neighbors=5, weights=\"uniform\", metric=\"minkowski\", p=2))\n",
    "])\n",
    "knn_sklearn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d140a7",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "We’ll tune n_neighbors, weights, and metric with 5-fold Stratified CV on the sklearn pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fc9025",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    \"knn__n_neighbors\": [3, 5, 7, 9, 11],\n",
    "    \"knn__weights\": [\"uniform\", \"distance\"],\n",
    "    \"knn__metric\": [\"minkowski\"],  # (p=1 manhattan, p=2 euclidean); you can add \"chebyshev\" etc.\n",
    "    \"knn__p\": [1, 2],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"knn\", KNeighborsClassifier())\n",
    "    ]),\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"Best CV accuracy:\", grid.best_score_)\n",
    "knn_best = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3f2f1a",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We’ll compute accuracy, confusion matrices, classification reports, and multi-class ROC-AUC (OvR).\n",
    "We’ll also draw a simple k-selection curve to visualize the effect of n_neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451dc253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multiclass(name: str, model, X_te, y_te, label_names=None):\n",
    "    y_pred = model.predict(X_te)\n",
    "    acc = accuracy_score(y_te, y_pred)\n",
    "\n",
    "    # Try to get class probabilities for macro ROC-AUC (OvR)\n",
    "    auc = np.nan\n",
    "    try:\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            proba = model.predict_proba(X_te)\n",
    "        else:\n",
    "            # fall back: if this is the simple KNN used with pre-scaled arrays\n",
    "            # wrap to supply predict_proba\n",
    "            proba = None\n",
    "        if proba is not None:\n",
    "            auc = roc_auc_score(y_te, proba, multi_class=\"ovr\", average=\"macro\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Macro ROC-AUC (OvR): {auc if not np.isnan(auc) else 'N/A'}\")\n",
    "    print(\"Confusion matrix:\\n\", confusion_matrix(y_te, y_pred))\n",
    "    print(\"Classification report:\\n\", classification_report(y_te, y_pred, target_names=label_names))\n",
    "    return acc, auc\n",
    "\n",
    "# Evaluate from-scratch (note: we must pass scaled arrays)\n",
    "acc_simple, auc_simple = evaluate_multiclass(\n",
    "    \"SimpleKNN (from-scratch, scaled)\", knn_simple, X_test_scaled, y_test, wine.target_names\n",
    ")\n",
    "\n",
    "# Evaluate sklearn baseline\n",
    "acc_sk, auc_sk = evaluate_multiclass(\n",
    "    \"KNeighbors (sklearn)\", knn_sklearn, X_test, y_test, wine.target_names\n",
    ")\n",
    "\n",
    "# Evaluate tuned model\n",
    "acc_best, auc_best = evaluate_multiclass(\n",
    "    \"KNeighbors (tuned)\", knn_best, X_test, y_test, wine.target_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840b2652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-selection curve with cross-validation (on pipeline)\n",
    "k_values = list(range(1, 26, 2))  # odd k to reduce ties\n",
    "cv_scores = []\n",
    "pipe = Pipeline([(\"scaler\", StandardScaler()), (\"knn\", KNeighborsClassifier())])\n",
    "\n",
    "for k in k_values:\n",
    "    pipe.set_params(knn__n_neighbors=k, knn__weights=\"uniform\", knn__metric=\"minkowski\", knn__p=2)\n",
    "    scores = cross_val_score(pipe, X_train, y_train, cv=cv, scoring=\"accuracy\", n_jobs=-1)\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "plt.plot(k_values, cv_scores, marker=\"o\")\n",
    "plt.xlabel(\"k (n_neighbors)\")\n",
    "plt.ylabel(\"CV Accuracy (mean over folds)\")\n",
    "plt.title(\"k-selection curve (uniform, Euclidean)\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d91e37",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\t•\tKNN is a lazy, non-parametric learner: “training” just stores the data; prediction is distance-based lookup.\n",
    "\t•\tScaling is essential — feature magnitudes define distances.\n",
    "\t•\tHyperparameters to tune:\n",
    "\t•\tn_neighbors (k): typically odd numbers for binary; search grid or elbow curve\n",
    "\t•\tweights: \"uniform\" vs \"distance\"\n",
    "\t•\tmetric & p: Euclidean (p=2), Manhattan (p=1), etc.\n",
    "\t•\tComplexity: prediction is O(n \\cdot d) per query (n = train size, d = features). Use indexing (KD-trees/ball trees) or approximate methods for large n.\n",
    "\t•\tStrengths: simple, competitive with good scaling and k-choice, handles multi-class naturally.\n",
    "\t•\tWeaknesses: slow at inference for large datasets; sensitive to irrelevant features and scale."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
